{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "115204b1-3fc1-4b77-9236-35bf78f08a47",
   "metadata": {},
   "source": [
    "# Age and Gender Classification\n",
    "\n",
    "This notebook demonstrates the process of loading a pretrained Wav2Vec2 model, performing dynamic quantization, and evaluating the model on an audio file for age and gender classification.\n",
    "\n",
    "## Imports and Model Definition\n",
    "\n",
    "First, we import the necessary libraries and define the ModelHead and AgeGenderModel classes, which extend the pretrained Wav2Vec2 model with custom heads for age and gender classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d9f0d17a-15df-424c-9484-42437dbde88e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import Wav2Vec2Processor, Wav2Vec2Model, Wav2Vec2Config, Wav2Vec2PreTrainedModel\n",
    "import soundfile as sf\n",
    "import torchaudio\n",
    "\n",
    "class ModelHead(nn.Module):\n",
    "    def __init__(self, config, num_labels):\n",
    "        super().__init__()\n",
    "        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
    "        self.dropout = nn.Dropout(config.final_dropout)\n",
    "        self.out_proj = nn.Linear(config.hidden_size, num_labels)\n",
    "\n",
    "    def forward(self, features, **kwargs):\n",
    "        x = features\n",
    "        x = self.dropout(x)\n",
    "        x = self.dense(x)\n",
    "        x = torch.tanh(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.out_proj(x)\n",
    "        return x\n",
    "\n",
    "class AgeGenderModel(Wav2Vec2PreTrainedModel):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.config = config\n",
    "        self.wav2vec2 = Wav2Vec2Model(config)\n",
    "        self.age = ModelHead(config, 1)\n",
    "        self.gender = ModelHead(config, 3)\n",
    "        self.init_weights()\n",
    "\n",
    "    def forward(self, input_values):\n",
    "        outputs = self.wav2vec2(input_values)\n",
    "        hidden_states = outputs[0]\n",
    "        hidden_states = torch.mean(hidden_states, dim=1)\n",
    "        logits_age = self.age(hidden_states)\n",
    "        logits_gender = self.gender(hidden_states)\n",
    "        return logits_age, logits_gender"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "741985d8-6a51-4430-8f38-04fcd31f2f64",
   "metadata": {},
   "source": [
    "## Loading and Saving the Original Model\n",
    "\n",
    "Next, we load the pretrained Wav2Vec2 model and save its state dictionary. This step ensures we have a backup of the original model before applying quantization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "acefb40d-065a-44d6-ac9d-8dc1f27be530",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of AgeGenderModel were not initialized from the model checkpoint at audeering/wav2vec2-large-robust-24-ft-age-gender and are newly initialized: ['wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Load model from hub\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "cpu_device = torch.device('cpu')\n",
    "model_name = 'audeering/wav2vec2-large-robust-24-ft-age-gender'\n",
    "processor = Wav2Vec2Processor.from_pretrained(model_name)\n",
    "config = Wav2Vec2Config.from_pretrained(model_name)\n",
    "\n",
    "# Load the original model\n",
    "model = AgeGenderModel.from_pretrained(model_name, config=config)\n",
    "model.to(cpu_device)  # Move to CPU for quantization\n",
    "\n",
    "# Save the original model - We will not push this to the git repo as it's too large\n",
    "original_model_path = \"age_gender_model.pth\"\n",
    "torch.save(model.state_dict(), original_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ecc784d-b998-4c14-9e52-f53cb1bf7105",
   "metadata": {},
   "source": [
    "## Dynamic Quantization\n",
    "\n",
    "We apply dynamic quantization to the model to reduce its size. Quantization converts the model weights from floating-point to integer representation, which can lead to significant reductions in model size and potential improvements in inference speed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b9c8ba2a-4704-4521-a6c9-59b394866443",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original model size: 1211.49 MB\n",
      "Quantized model size: 340.10 MB\n"
     ]
    }
   ],
   "source": [
    "# Ensure the quantization engine is set\n",
    "torch.backends.quantized.engine = 'qnnpack'\n",
    "\n",
    "# Apply dynamic quantization\n",
    "quantized_model = torch.quantization.quantize_dynamic(\n",
    "    model, {torch.nn.Linear}, dtype=torch.qint8\n",
    ")\n",
    "\n",
    "# Save the quantized model\n",
    "quantized_model_path = \"quantized_age_gender_model.pth\"\n",
    "torch.save(quantized_model.state_dict(), quantized_model_path)\n",
    "\n",
    "# Verify model size reduction\n",
    "original_model_size = os.path.getsize(original_model_path) / (1024 * 1024)\n",
    "quantized_model_size = os.path.getsize(quantized_model_path) / (1024 * 1024)\n",
    "print(f\"Original model size: {original_model_size:.2f} MB\")\n",
    "print(f\"Quantized model size: {quantized_model_size:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2774e60d-d247-4071-acec-e1e334d91845",
   "metadata": {},
   "source": [
    "## Audio Preprocessing\n",
    "\n",
    "We define helper functions to resample and normalize audio signals to ensure they are in the correct format for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "62fed028-93ec-45cb-86f1-6b9ee5a5ba40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure sampling rate is 16,000 Hz\n",
    "TARGET_SAMPLING_RATE = 16000\n",
    "\n",
    "def resample_audio(signal, orig_sr, target_sr):\n",
    "    if orig_sr != target_sr:\n",
    "        resampler = torchaudio.transforms.Resample(orig_sr, target_sr)\n",
    "        signal = resampler(torch.tensor(signal).float())\n",
    "    return signal.numpy()\n",
    "\n",
    "def normalize_audio(signal):\n",
    "    return (signal - np.mean(signal)) / np.std(signal)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "258b541a-9f10-4d0c-a668-f6ae3e1ff1a3",
   "metadata": {},
   "source": [
    "## Model Inference\n",
    "\n",
    "This function processes an audio file and uses the model to predict the age and gender. The audio is resampled, normalized, and passed through the Wav2Vec2 processor before being fed into the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e4fe9cc9-0d0e-4c28-bc99-3031ca3d0aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_func(model, file_path: str):\n",
    "    signal, sr = sf.read(file_path)\n",
    "    if len(signal.shape) > 1:\n",
    "        signal = np.mean(signal, axis=1)  # Convert to mono\n",
    "    signal = resample_audio(signal, sr, TARGET_SAMPLING_RATE)\n",
    "    signal = normalize_audio(signal)\n",
    "    inputs = processor(signal, sampling_rate=TARGET_SAMPLING_RATE, return_tensors=\"pt\", padding=True)\n",
    "    inputs = inputs.to(cpu_device)  # Ensure processing on CPU\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        logits_age, logits_gender = model(inputs['input_values'])\n",
    "        \n",
    "        # Apply scaling to the age logits\n",
    "        age = round(logits_age.item() * 100)  # Assuming a scale factor of 100 for interpretation\n",
    "        \n",
    "        gender_probs = torch.softmax(logits_gender, dim=1).cpu().numpy()[0]\n",
    "        gender = ['female', 'male', 'child'][np.argmax(gender_probs)]\n",
    "        \n",
    "    return age, gender, gender_probs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6381b8f8-fec3-45b8-b133-374346ff5014",
   "metadata": {},
   "source": [
    "## Testing the Model\n",
    "\n",
    "Finally, we test the model using an audio file. We perform inference using both the original and quantized models to compare their outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3101983b-1f41-4f68-a1d9-a8a9ecc4dbae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Model - File: 01.wav, Age: 37, Gender: female (Probs: [9.944589e-01 4.861043e-03 6.799646e-04])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of AgeGenderModel were not initialized from the model checkpoint at audeering/wav2vec2-large-robust-24-ft-age-gender and are newly initialized: ['wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/opt/anaconda3/envs/ptenv/lib/python3.9/site-packages/torch/_utils.py:383: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  device=storage.device,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantized Model - File: 01.wav, Age: 37, Gender: female (Probs: [9.944589e-01 4.861043e-03 6.799646e-04])\n"
     ]
    }
   ],
   "source": [
    "# Path to the specific file to test\n",
    "file_path = 'all_recordings/01.wav'\n",
    "\n",
    "# Predict age and gender using the original model\n",
    "age_original, gender_original, gender_probs_original = process_func(model, file_path)\n",
    "print(f\"Original Model - File: {os.path.basename(file_path)}, Age: {age_original}, Gender: {gender_original} (Probs: {gender_probs_original})\")\n",
    "\n",
    "# Load the quantized model for prediction\n",
    "quantized_model = AgeGenderModel.from_pretrained(model_name, config=config)\n",
    "quantized_model.load_state_dict(torch.load(quantized_model_path, map_location=cpu_device), strict=False)\n",
    "quantized_model.to(cpu_device)  # Ensure quantized model is on CPU\n",
    "\n",
    "# Predict age and gender using the quantized model\n",
    "age_quantized, gender_quantized, gender_probs_quantized = process_func(quantized_model, file_path)\n",
    "print(f\"Quantized Model - File: {os.path.basename(file_path)}, Age: {age_quantized}, Gender: {gender_quantized} (Probs: {gender_probs_quantized})\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
