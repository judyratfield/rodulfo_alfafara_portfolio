{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "716e7a89",
   "metadata": {},
   "source": [
    "# Classify \n",
    "This notebook combines all other solutions to determine whether a caller is angry, and to which agent the angry caller will be assigned to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7f98c55e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nbformat\n",
    "from IPython import get_ipython\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "\n",
    "def import_from_notebook(notebook_path, function_name):\n",
    "    with open(notebook_path, 'r', encoding='utf-8') as f:\n",
    "        nb = nbformat.read(f, as_version=4)\n",
    "\n",
    "    shell = InteractiveShell.instance()\n",
    "    code = \"\\n\".join([cell.source for cell in nb.cells if cell.cell_type == 'code'])\n",
    "    exec(code, shell.user_ns)\n",
    "    return shell.user_ns[function_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "55af825d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.5.2 (SDL 2.28.3, Python 3.9.16)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "is_caller_angry = import_from_notebook('Text Semantic Analysis.ipynb', 'is_caller_angry')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "599e7c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import librosa\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from keras.models import load_model\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "\n",
    "# Load and process the audio files in a folder\n",
    "def load_audio_file(file_path):\n",
    "    data, sampling_rate = librosa.load(file_path, duration=2.5, offset=0.6)\n",
    "    return data, sampling_rate\n",
    "\n",
    "# Feature extraction function\n",
    "def extract_features(data, sample_rate):\n",
    "    result = np.array([])\n",
    "    \n",
    "    # Zero Crossing Rate\n",
    "    zcr = np.mean(librosa.feature.zero_crossing_rate(y=data).T, axis=0)\n",
    "    result = np.hstack((result, zcr))\n",
    "    \n",
    "    # Chroma STFT\n",
    "    stft = np.abs(librosa.stft(data))\n",
    "    chroma_stft = np.mean(librosa.feature.chroma_stft(S=stft, sr=sample_rate).T, axis=0)\n",
    "    result = np.hstack((result, chroma_stft))\n",
    "    \n",
    "    # MFCC\n",
    "    mfcc = np.mean(librosa.feature.mfcc(y=data, sr=sample_rate).T, axis=0)\n",
    "    result = np.hstack((result, mfcc))\n",
    "    \n",
    "    # Root Mean Square Value\n",
    "    rms = np.mean(librosa.feature.rms(y=data).T, axis=0)\n",
    "    result = np.hstack((result, rms))\n",
    "    \n",
    "    # Mel Spectrogram\n",
    "    mel = np.mean(librosa.feature.melspectrogram(y=data, sr=sample_rate).T, axis=0)\n",
    "    result = np.hstack((result, mel))\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Function to preprocess and standardize data\n",
    "def preprocess_data(features, scaler):\n",
    "    features = scaler.transform(features.reshape(1, -1))\n",
    "    features = np.expand_dims(features, axis=2)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "856669e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the saved model\n",
    "model_emotion = load_model('emotion_recognition_model.h5')\n",
    "\n",
    "def get_emotion(file_path):\n",
    "    \n",
    "    # Load the scaler used for training\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit_transform(pd.read_csv('features.csv').iloc[:, :-1].values)\n",
    "\n",
    "    file_path = file_path\n",
    "\n",
    "    # Load the label encoder used for training\n",
    "    label_encoder = OneHotEncoder()\n",
    "    label_encoder.fit(pd.read_csv('features.csv')['labels'].values.reshape(-1, 1))\n",
    "\n",
    "    # Load and preprocess the audio file\n",
    "    data, sampling_rate = load_audio_file(file_path)\n",
    "    features = extract_features(data, sampling_rate)\n",
    "    processed_features = preprocess_data(features, scaler)\n",
    "\n",
    "    # Make a prediction\n",
    "    prediction = model_emotion.predict(processed_features)\n",
    "    predicted_label = np.argmax(prediction, axis=1)\n",
    "\n",
    "    # Map the predicted label to the corresponding emotion\n",
    "    emotion = label_encoder.categories_[0][predicted_label[0]]\n",
    "\n",
    "    return emotion\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0a3bd88a",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_country_dict = {\n",
    "    'african': 'Caller assigned to African agent.',\n",
    "    'australia': 'Caller assigned to Australian agent.',\n",
    "    'bermuda': 'Caller assigned to Bermudian agent.',\n",
    "    'canada': 'Caller assigned to Canadian agent.',\n",
    "    'england': 'Caller assigned to English agent.',\n",
    "    'hongkong': 'Caller assigned to Hong Kong agent.',\n",
    "    'indian': 'Caller assigned to Indian agent.',\n",
    "    'ireland': 'Caller assigned to Irish agent.',\n",
    "    'malaysia': 'Caller assigned to Malaysian agent.',\n",
    "    'newzealand': 'Caller assigned to New Zealand agent.',\n",
    "    'philippines': 'Caller assigned to Filipino agent.',\n",
    "    'scotland': 'Caller assigned to Scottish agent.',\n",
    "    'singapore': 'Caller assigned to Singaporean agent.',\n",
    "    'southatlandtic': 'Caller assigned to South Atlantic agent.',\n",
    "    'us': 'Caller assigned to US agent.',\n",
    "    'wales': 'Caller assigned to Welsh agent.'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6f11a095-88f8-48ba-a6c1-0ac49b3a332a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/ptenv/lib/python3.9/site-packages/torch/_utils.py:383: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  device=storage.device,\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import Wav2Vec2Processor, Wav2Vec2Model, Wav2Vec2Config, Wav2Vec2PreTrainedModel\n",
    "import soundfile as sf\n",
    "import torchaudio\n",
    "\n",
    "# Define the ModelHead and AgeGenderModel classes\n",
    "class ModelHead(nn.Module):\n",
    "    def __init__(self, config, num_labels):\n",
    "        super().__init__()\n",
    "        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
    "        self.dropout = nn.Dropout(config.final_dropout)\n",
    "        self.out_proj = nn.Linear(config.hidden_size, num_labels)\n",
    "\n",
    "    def forward(self, features, **kwargs):\n",
    "        x = features\n",
    "        x = self.dropout(x)\n",
    "        x = self.dense(x)\n",
    "        x = torch.tanh(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.out_proj(x)\n",
    "        return x\n",
    "\n",
    "class AgeGenderModel(Wav2Vec2PreTrainedModel):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.config = config\n",
    "        self.wav2vec2 = Wav2Vec2Model(config)\n",
    "        self.age = ModelHead(config, 1)\n",
    "        self.gender = ModelHead(config, 3)\n",
    "        self.init_weights()\n",
    "\n",
    "    def forward(self, input_values):\n",
    "        outputs = self.wav2vec2(input_values)\n",
    "        hidden_states = outputs[0]\n",
    "        hidden_states = torch.mean(hidden_states, dim=1)\n",
    "        logits_age = self.age(hidden_states)\n",
    "        logits_gender = self.gender(hidden_states)\n",
    "        return logits_age, logits_gender\n",
    "\n",
    "# Load model and processor\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model_name = 'audeering/wav2vec2-large-robust-24-ft-age-gender'\n",
    "processor = Wav2Vec2Processor.from_pretrained(model_name)\n",
    "\n",
    "# Load the saved age and gender model\n",
    "cpu_device = torch.device('cpu')\n",
    "age_gender_model_path = 'quantized_age_gender_model.pth'\n",
    "config = Wav2Vec2Config.from_pretrained(model_name)\n",
    "model = AgeGenderModel(config)\n",
    "model.load_state_dict(torch.load(age_gender_model_path, map_location=cpu_device), strict=False)\n",
    "model.to(device)\n",
    "\n",
    "# Ensure sampling rate is 16,000 Hz\n",
    "TARGET_SAMPLING_RATE = 16000\n",
    "\n",
    "def resample_audio(signal, orig_sr, target_sr):\n",
    "    if orig_sr != target_sr:\n",
    "        resampler = torchaudio.transforms.Resample(orig_sr, target_sr)\n",
    "        signal = resampler(torch.tensor(signal).float())\n",
    "    return signal.numpy()\n",
    "\n",
    "def normalize_audio(signal):\n",
    "    return (signal - np.mean(signal)) / np.std(signal)\n",
    "\n",
    "def process_func(file_path: str):\n",
    "    signal, sr = sf.read(file_path)\n",
    "    if len(signal.shape) > 1:\n",
    "        signal = np.mean(signal, axis=1)  # Convert to mono\n",
    "    signal = resample_audio(signal, sr, TARGET_SAMPLING_RATE)\n",
    "    signal = normalize_audio(signal)\n",
    "    inputs = processor(signal, sampling_rate=TARGET_SAMPLING_RATE, return_tensors=\"pt\", padding=True)\n",
    "    inputs = inputs.to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        logits_age, logits_gender = model(inputs['input_values'])\n",
    "        \n",
    "        # Apply scaling to the age logits\n",
    "        age = round(logits_age.item() * 100)  # Assuming a scale factor of 100 for interpretation\n",
    "        \n",
    "        gender_probs = torch.softmax(logits_gender, dim=1).cpu().numpy()[0]\n",
    "        gender = ['female', 'male', 'child'][np.argmax(gender_probs)]\n",
    "        \n",
    "    return age, gender, gender_probs\n",
    "\n",
    "import torch\n",
    "import torchaudio\n",
    "from transformers import AutoFeatureExtractor, AutoModelForAudioClassification\n",
    "\n",
    "# Load the saved model for accent classification\n",
    "model_name = \"english_accents_classification\"\n",
    "model_endorse = AutoModelForAudioClassification.from_pretrained(model_name)\n",
    "\n",
    "# Move the model to the appropriate device (CPU or GPU)\n",
    "model_endorse.to(device)\n",
    "\n",
    "# Load the feature extractor\n",
    "feature_extractor = AutoFeatureExtractor.from_pretrained(\"dima806/english_accents_classification\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "05eff7e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "from transformers import AutoFeatureExtractor, AutoModelForAudioClassification, pipeline, TrainingArguments, Trainer\n",
    "\n",
    "# Load the saved model\n",
    "model_name = \"english_accents_classification\"\n",
    "model_endorse = AutoModelForAudioClassification.from_pretrained(model_name)\n",
    "\n",
    "# Move the model to the appropriate device (CPU or GPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_endorse.to(device)\n",
    "\n",
    "# Load the feature extractor\n",
    "feature_extractor = AutoFeatureExtractor.from_pretrained(\"dima806/english_accents_classification\")\n",
    "\n",
    "def endorse_to_agent(file_path):\n",
    "    RATE_HZ = 16000\n",
    "\n",
    "    audio, rate = torchaudio.load(file_path)\n",
    "    transform = torchaudio.transforms.Resample(rate, RATE_HZ)\n",
    "    audio = transform(audio).numpy().reshape(-1)\n",
    "\n",
    "    target_sample_rate = feature_extractor.sampling_rate\n",
    "\n",
    "    # Preprocess the audio\n",
    "    inputs = feature_extractor(audio, sampling_rate=target_sample_rate, return_tensors=\"pt\")\n",
    "\n",
    "    # Move inputs to the same device as the model\n",
    "    inputs = {key: value.to(device) for key, value in inputs.items()}\n",
    "\n",
    "    # Make predictions\n",
    "    with torch.no_grad():\n",
    "        outputs = model_endorse(**inputs)\n",
    "\n",
    "    # Extract logits (raw predictions)\n",
    "    logits = outputs.logits\n",
    "\n",
    "    # Get predicted class\n",
    "    predicted_class_id = logits.argmax().item()\n",
    "    predicted_class_label = model_endorse.config.id2label[predicted_class_id]\n",
    "\n",
    "    # Age and Gender prediction\n",
    "    age, gender, gender_probs = process_func(file_path)\n",
    "\n",
    "    # Gender-swap endorsement logic\n",
    "    if gender == 'male' or gender == 'child':\n",
    "        agent_gender = 'female'\n",
    "    elif gender == 'female' or gender == 'child':\n",
    "        agent_gender = 'male'\n",
    "\n",
    "    # Age-based endorsement logic\n",
    "    if age <= 30:\n",
    "        agent_age = '30 and below'\n",
    "    elif age <= 40:\n",
    "        agent_age = '40 and below'\n",
    "    elif age <= 50:\n",
    "        agent_age = '50 and below'\n",
    "    elif age <= 60:\n",
    "        agent_age = '60 and below'\n",
    "    else:\n",
    "        agent_age = 'above 60'\n",
    "\n",
    "    agent_age_gender_info = f\"Caller assigned to age {agent_age}, {agent_gender} agent.\"\n",
    "\n",
    "    return agent_country_dict[predicted_class_label], agent_age_gender_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6ae5201c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#runs customer to all checkings if he truly is not angry\n",
    "\n",
    "def verify_customer_is_calm(file_path):\n",
    "    transcript_result = is_caller_angry(file_path)\n",
    "    emotion = get_emotion(file_path)\n",
    "    if emotion == 'angry' or transcript_result == 'negative':\n",
    "        endorsement = endorse_to_agent(file_path)\n",
    "        print('customer is angry')\n",
    "        print(endorsement)\n",
    "        return 1\n",
    "    else:\n",
    "        print('customer is calm')\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50f20850",
   "metadata": {},
   "source": [
    "### verify_customer_is_calm (Neural Network Version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4af42203",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import librosa\n",
    "import librosa.display\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "import seaborn as sns\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv1D, MaxPooling1D, Flatten, Dropout\n",
    "import os\n",
    "import pandas as pd\n",
    "import warnings\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9fc19c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Audio data augmentation functions\n",
    "def noise(data):\n",
    "    noise_amp = 0.035 * np.random.uniform() * np.amax(data)\n",
    "    data = data + noise_amp * np.random.normal(size=data.shape[0])\n",
    "    return data\n",
    "\n",
    "def stretch(data, rate=0.8):\n",
    "    return librosa.effects.time_stretch(data, rate=rate)\n",
    "\n",
    "def shift(data):\n",
    "    shift_range = int(np.random.uniform(low=-5, high=5) * 1000)\n",
    "    return np.roll(data, shift_range)\n",
    "\n",
    "def pitch(data, sampling_rate, pitch_factor=0.7):\n",
    "    return librosa.effects.pitch_shift(data, sr=sampling_rate, n_steps=pitch_factor)\n",
    "\n",
    "# Feature extraction function\n",
    "def extract_features(data, sample_rate):\n",
    "    result = np.array([])\n",
    "    \n",
    "    # Zero Crossing Rate\n",
    "    zcr = np.mean(librosa.feature.zero_crossing_rate(y=data).T, axis=0)\n",
    "    result = np.hstack((result, zcr))\n",
    "    \n",
    "    # Chroma STFT\n",
    "    stft = np.abs(librosa.stft(data))\n",
    "    chroma_stft = np.mean(librosa.feature.chroma_stft(S=stft, sr=sample_rate).T, axis=0)\n",
    "    result = np.hstack((result, chroma_stft))\n",
    "    \n",
    "    # MFCC\n",
    "    mfcc = np.mean(librosa.feature.mfcc(y=data, sr=sample_rate).T, axis=0)\n",
    "    result = np.hstack((result, mfcc))\n",
    "    \n",
    "    # Root Mean Square Value\n",
    "    rms = np.mean(librosa.feature.rms(y=data).T, axis=0)\n",
    "    result = np.hstack((result, rms))\n",
    "    \n",
    "    # Mel Spectrogram\n",
    "    mel = np.mean(librosa.feature.melspectrogram(y=data, sr=sample_rate).T, axis=0)\n",
    "    result = np.hstack((result, mel))\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Function to get features from an audio file path\n",
    "def get_features(path):\n",
    "    data, sample_rate = librosa.load(path, duration=2.5, offset=0.6)\n",
    "    \n",
    "    # Without augmentation\n",
    "    res1 = extract_features(data, sample_rate)\n",
    "    result = np.array(res1)\n",
    "    \n",
    "    # Data with noise\n",
    "    noise_data = noise(data)\n",
    "    res2 = extract_features(noise_data, sample_rate)\n",
    "    result = np.vstack((result, res2))\n",
    "    \n",
    "    # Data with stretching and pitching\n",
    "    new_data = stretch(data)\n",
    "    data_stretch_pitch = pitch(new_data, sample_rate)\n",
    "    res3 = extract_features(data_stretch_pitch, sample_rate)\n",
    "    result = np.vstack((result, res3))\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9dac397e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "#imported necessary packages\n",
    "from pydub import AudioSegment\n",
    "import pygame\n",
    "\n",
    "#defined function that plays an audio file\n",
    "\n",
    "def play_audio(file_path):\n",
    "    pygame.init()\n",
    "    pygame.mixer.init()\n",
    "    try:\n",
    "        pygame.mixer.music.load(file_path)#loads the audio file\n",
    "        pygame.mixer.music.play()#plays the audio file\n",
    "        while pygame.mixer.music.get_busy():#waits until the audio playback is finished before continuing with the rest of the program\n",
    "            pygame.time.Clock().tick(10)\n",
    "    except pygame.error as e:\n",
    "        print(\"Error occurred while playing audio:\", e) #prints an error message should the program fails to be executed\n",
    "    pygame.quit()\n",
    "    \n",
    "#defined function that can play audio through the speakers (or headphones)\n",
    "\n",
    "def play(input_file):\n",
    "    #called function that plays the newly converted wav audio file\n",
    "    play_audio(input_file)\n",
    "    \n",
    "#imported necessary packages\n",
    "from transformers import pipeline\n",
    "\n",
    "#Using pipeline, created a transcriber that when called, converts an audio file and transcribes it into text\n",
    "transcriber = pipeline(task=\"automatic-speech-recognition\", model=\"openai/whisper-small\")\n",
    "\n",
    "#imported logging so we can log the transcript into a log file\n",
    "import logging\n",
    "\n",
    "import ffmpeg\n",
    "\n",
    "#Configures logging to write logs to a file named transcript.log\n",
    "logging.basicConfig(filename='transcript.log', level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "#defined a function that would utilize the transcriber\n",
    "#in case the transcriber fails, the user will be asked to manually type the script\n",
    "def transcribe(input_file):\n",
    "    print(\"Transcribing. Please wait...\")\n",
    "    try:\n",
    "        transcript = transcriber(input_file)#calls the transcriber\n",
    "        logging.info(\"Transcript: %s\", transcript)  #Logs the transcript\n",
    "        print(\"Transcript:\", transcript)  #Prints the transcript\n",
    "    except Exception as e:#should transcriber fail, the messages below will be displayed and the user will be prompted to enter transcript manually\n",
    "        logging.error(\"Error occurred: %s\", e)  # Log the error\n",
    "        print(\"Error occurred:\", e)  # Print the error message\n",
    "\n",
    "    return transcript\n",
    "\n",
    "#defined function that determines whether a customer is angry or not\n",
    "\n",
    "def get_semantic_score(input_file):\n",
    "    #called transcribe function\n",
    "    transcript = transcribe(input_file)\n",
    "    \n",
    "    from transformers import pipeline\n",
    "    \n",
    "    classify_sentiment = pipeline(\"text-classification\", model=\"cardiffnlp/twitter-roberta-base-sentiment-latest\")\n",
    "\n",
    "    try:\n",
    "        sentiment = classify_sentiment(transcript['text'])\n",
    "        score = sentiment[0]['score'] #assigns score to score variable\n",
    "\n",
    "    except:\n",
    "        sentiment = 'negative'\n",
    "\n",
    "    #extracts the label from the sentiment\n",
    "    try:\n",
    "        sentiment_label = sentiment[0]['label']\n",
    "    except:\n",
    "        sentiment_label = sentiment\n",
    "\n",
    "    return score, sentiment_label\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0595b770",
   "metadata": {},
   "outputs": [],
   "source": [
    "#runs customer to all checkings if he truly is not angry\n",
    "\n",
    "from tensorflow.keras.models import load_model\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "# Load the best model\n",
    "best_model = load_model('best_model.keras')\n",
    "\n",
    "# Load the CSV file\n",
    "csv_file = 'recordings_for_testing.csv'\n",
    "df = pd.read_csv(csv_file)\n",
    "\n",
    "def nn_verify_customer_is_calm(file_path):\n",
    "    \n",
    "    # Data Preparation\n",
    "    X, Y = [], []\n",
    "\n",
    "    filename = os.path.basename(file_path)\n",
    "\n",
    "    emotion = df.loc[df['Filename'] == filename, 'Labels'].iloc[0]\n",
    "\n",
    "    feature = get_features(file_path)\n",
    "    for ele in feature:\n",
    "        X.append(ele)\n",
    "        Y.append(emotion)\n",
    "\n",
    "    # Convert features and labels to DataFrame\n",
    "    Features = pd.DataFrame(X)\n",
    "    Features['labels'] = Y\n",
    "\n",
    "    results = []\n",
    "\n",
    "    score, sentiment_label = get_semantic_score(file_path)\n",
    "    results.append((filename, score, sentiment_label))\n",
    "\n",
    "    # Create a DataFrame from the results\n",
    "    sentiment_df = pd.DataFrame(results, columns=['filename', 'score', 'sentiment_label'])\n",
    "\n",
    "\n",
    "\n",
    "    # Convert the 'score' and 'sentiment_label' columns to tensors\n",
    "    scores_tensor = tf.convert_to_tensor(sentiment_df['score'].values, dtype=tf.float32)\n",
    "\n",
    "    # Mapping sentiment labels to integers \n",
    "    sentiment_mapping = {'neutral': 0, 'negative': 1, 'positive': 2}\n",
    "    sentiments_tensor = tf.convert_to_tensor(sentiment_df['sentiment_label'].map(sentiment_mapping).values, dtype=tf.int64)\n",
    "\n",
    "    # Convert audio features to tensor\n",
    "    audio_features_tensor = tf.convert_to_tensor(Features.drop(columns=['labels']).values, dtype=tf.float32)\n",
    "    labels_tensor = tf.convert_to_tensor(Features['labels'].values, dtype=tf.int64)\n",
    "    \n",
    "    # Repeat the sentiment scores and labels to match the augmented feature sets\n",
    "    expanded_scores_tensor = tf.repeat(scores_tensor, repeats=3)\n",
    "    expanded_sentiments_tensor = tf.repeat(sentiments_tensor, repeats=3)\n",
    "\n",
    "    # Convert sentiments tensor to float32\n",
    "    expanded_sentiments_tensor = tf.cast(expanded_sentiments_tensor, dtype=tf.float32)\n",
    "\n",
    "    # Combine sentiment scores and labels with audio features\n",
    "    expanded_scores_tensor = tf.expand_dims(expanded_scores_tensor, axis=1)\n",
    "    expanded_sentiments_tensor = tf.expand_dims(expanded_sentiments_tensor, axis=1)\n",
    "    combined_features_tensor = tf.concat([audio_features_tensor, expanded_scores_tensor, expanded_sentiments_tensor], axis=1)\n",
    "\n",
    "    # Use the model to make predictions\n",
    "    label = int(max(best_model.predict(combined_features_tensor)))\n",
    "\n",
    "    if label == 1:\n",
    "        endorsement = endorse_to_agent(file_path)\n",
    "        print('customer is angry')\n",
    "        print(endorsement)\n",
    "        return 1\n",
    "    else:\n",
    "        print('customer is calm')\n",
    "        return 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dd0eff04-e68e-4851-9415-cd1a83f83efd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcribing. Please wait...\n",
      "Transcript: {'text': ' Oh, you got go'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "customer is angry\n",
      "('Caller assigned to English agent.', 'Caller assigned to age 30 and below, female agent.')\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "# Test - Angry caller: English 30s male\n",
    "print(nn_verify_customer_is_calm('all_recordings/angry1.wav'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "30ea4267-7505-43fc-b603-8256945c5e1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcribing. Please wait...\n",
      "Transcript: {'text': ' I hate you.'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step\n",
      "customer is angry\n",
      "('Caller assigned to English agent.', 'Caller assigned to age 30 and below, female agent.')\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "# Test - Angry caller: English 30s male\n",
    "print(nn_verify_customer_is_calm('all_recordings/angry22.wav'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2c149e59-b7f8-4869-b2c2-bd8ec39fd5c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcribing. Please wait...\n",
      "Transcript: {'text': ' My issue has not been resolved. What are the next steps?'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "customer is calm\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# Test - Calm caller: English 20s female\n",
    "print(nn_verify_customer_is_calm('all_recordings/10.wav'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "692432f9-dd9c-465f-ad65-e6b272a61504",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
